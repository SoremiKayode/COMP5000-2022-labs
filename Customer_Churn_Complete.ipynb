{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFUXDU5tlYa++2NrmZx95V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoremiKayode/COMP5000-2022-labs/blob/main/Customer_Churn_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Customer Churn Prediction For a Telecommunication Network**\n",
        ">Telecommunication customer churn prediction plays a crucial role in reducing customer attrition rates and improving customer retention strategies. This study focuses on utilizing machine learning algorithms, namely Logistic Regression, K-Nearest Neighbours (KNN), Support Vector Machine (SVM), and Deep Learning, to predict customer churn in the telecommunication industry. The research incorporates feature engineering techniques, such as removing irrelevant data and up sampling the dataset to achieve class balance. The main objective is to evaluate the performance of these algorithms and conduct parameter tuning for enhanced results.\n",
        "\n",
        ">The experimental results demonstrate that SVM achieves an accuracy of 89.9%, precision of 89.9%, and recall of 89.9% in predicting customer churn. The KNN model achieves an accuracy of 78%, precision of 80%, and recall of 80%. The optimized version of Logistic Regression yields an accuracy of 77.1%, precision of 77.2%, and recall of 77.1%. The Deep Learning model achieves an accuracy, precision, and recall of 85%.\n",
        "\n",
        ">The parameter tuning process involves optimizing the hyperparameters of each algorithm. For SVM, parameters such as the kernel type, regularization parameter, and gamma value are fine-tuned. In KNN, the number of neighbours is optimized. Logistic Regression is optimized by tuning the regularization parameter and learning rate. Deep Learning involves tuning parameters such as the number of layers, activation functions, and learning rate.\n",
        "\n",
        ">The findings of this research suggest that SVM outperforms other algorithms in terms of accuracy, precision, and recall. However, KNN, Logistic Regression, and Deep Learning also provide reasonably good results. The study highlights the significance of feature engineering and parameter tuning in improving the performance of machine learning algorithms for customer churn prediction in the telecommunication sector. These predictive models can assist telecommunication companies in identifying at-risk customers and implementing targeted retention strategies to reduce churn rates, enhance customer satisfaction, and ultimately improve business profitability.\n"
      ],
      "metadata": {
        "id": "ra67nBgwYU2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Outline of building the machine learning prediction model**.\n",
        "\n",
        "###**Data Analysis:**\n",
        "* Checking data summary and info\n",
        "* Removing unwanted column\n",
        "* working on missing data\n",
        "* Plotting the distribution of features with categorical data on an histogram\n",
        "* Checking for correlation among features of the dataset.\n",
        "\n",
        "###**Data Preprocessing**\n",
        "* one hot encoding categorical data using pandas pd.features\n",
        "* Scaling the input to zero and one using scikit learn MinMaxScaler\n",
        "* splitting dataset into training and testing.\n",
        "* searching for the best parameter to use."
      ],
      "metadata": {
        "id": "PoDXGR1W-U0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing the needed library**"
      ],
      "metadata": {
        "id": "xAqKGyN1EeHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras[tensorflow]\n",
        "pip install scikit-plot==0.3.7"
      ],
      "metadata": {
        "id": "GPTW5Pyyqx72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe8kYNYD4f5b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import importlib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, , PrecisionRecallDisplay, classification_report, confusion_matrix\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import scikitplot as skplt\n",
        "# from tensorflow.keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "%matplotlib inline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Loading the Dataset**"
      ],
      "metadata": {
        "id": "VhOBq-GMbdRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "data = pd.read_excel(\"drive/MyDrive/Colab Notebooks/Telco_customer_churn.xlsx\")\n",
        "data"
      ],
      "metadata": {
        "id": "zjpSAktIG2Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Getting the list of all the Features**"
      ],
      "metadata": {
        "id": "wSMLJjHQozdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column = data.columns\n",
        "print(len(column))\n",
        "column\n",
        "# The lenght of the columns is 33"
      ],
      "metadata": {
        "id": "OBcKBqfEH8xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Insight into the Dataset**"
      ],
      "metadata": {
        "id": "3zHjEtPFpJlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Data Summary\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "pF9a6FmiI2jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The data info is\")\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "gC9f0H3ZJizR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Handling Missing Data and missing column**\n",
        "\n",
        "It happens that there are no missing data in the dataset, except for the churn reason column, it will not be appropriate to include churn reason in the dataset, there is possibility for using the model to predict the churn possibility of cutomer who haven't spent much time with the company and definitely haven't churned, so we will drop the column alonside \"CustomerID', 'Count', 'Country', 'State', 'City', 'Zip Code', 'Lat Long', 'Latitude', and 'Longitude Churn Score, CLTV, Churn Reason' \"we want to focus on features that has high probability of affecting our predictions"
      ],
      "metadata": {
        "id": "x5TXkGIxKb1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wanted_columns = column[9:-3]\n",
        "data = data[wanted_columns]"
      ],
      "metadata": {
        "id": "e7ysJLc0MY9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a function to return the columns which are object and value count is not greater than 20"
      ],
      "metadata": {
        "id": "y6E9M07eOYDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categorical_columns(data):\n",
        "  cat_columns = []\n",
        "  for col in data.columns:\n",
        "    if data[col].dtype == \"O\" or data[col].value_counts().shape[0] < 20 :\n",
        "      cat_columns.append(col)\n",
        "  return cat_columns"
      ],
      "metadata": {
        "id": "jbDdBgFlR9zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = get_categorical_columns(data)"
      ],
      "metadata": {
        "id": "u-npAzcLUPbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing a function to plot the histogram of each categorical columns"
      ],
      "metadata": {
        "id": "tax5hCsCVbx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_categorical_columns(data, columns, arrangement):\n",
        "    len_columns = len(columns)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
        "\n",
        "    for y in range(len_columns):\n",
        "        sns.countplot(x=columns[y], data=data, ax=axes[arrangement[y][0], arrangement[y][1]])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "first_columns = categorical_columns[:6]\n",
        "second_columns = categorical_columns[6:12]\n",
        "third_columns = categorical_columns[12:18]\n",
        "arrangement = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
        "\n",
        "plot_all_categorical_columns(data, first_columns, arrangement)\n",
        "plot_all_categorical_columns(data, second_columns, arrangement)\n",
        "plot_all_categorical_columns(data, third_columns, arrangement)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLcTQxY-UXjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Plotting Continuous Data**"
      ],
      "metadata": {
        "id": "v2-jxXbTuIgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(12, 7))\n",
        "sns.lineplot(y=\"Tenure Months\", x=\"Churn Value\", data=data)\n",
        "sns.lineplot(y=\"Monthly Charges\", x=\"Churn Value\", data=data, ax=axes[0])"
      ],
      "metadata": {
        "id": "vpjvFurRuHFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Processing**"
      ],
      "metadata": {
        "id": "PxlbQnD0uQi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Removing unwanted Column**"
      ],
      "metadata": {
        "id": "TzixV51ruroF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Tenure Months',\n",
        "       'Phone Service', 'Multiple Lines', 'Internet Service',\n",
        "       'Online Security', 'Online Backup', 'Device Protection', 'Tech Support',\n",
        "       'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing',\n",
        "       'Payment Method', 'Monthly Charges', 'Total Charges',\n",
        "       'Churn Value']]"
      ],
      "metadata": {
        "id": "un0thw7Vxo8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Seperating Categorical Column**"
      ],
      "metadata": {
        "id": "r23fw7C7uzdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_column = [x for x in data.columns if data[x].dtype==\"O\"]"
      ],
      "metadata": {
        "id": "s4XQh8vcwhTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_column = ['Gender',\n",
        " 'Senior Citizen',\n",
        " 'Partner',\n",
        " 'Dependents',\n",
        " 'Phone Service',\n",
        " 'Multiple Lines',\n",
        " 'Internet Service',\n",
        " 'Online Security',\n",
        " 'Online Backup',\n",
        " 'Device Protection',\n",
        " 'Tech Support',\n",
        " 'Streaming TV',\n",
        " 'Streaming Movies',\n",
        " 'Contract',\n",
        " 'Paperless Billing',\n",
        " 'Payment Method']"
      ],
      "metadata": {
        "id": "euXZezBczbyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**One Hot Encoding**"
      ],
      "metadata": {
        "id": "bxfDqw3_xIuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(data=data, columns=cat_column, drop_first=True)"
      ],
      "metadata": {
        "id": "5iNsEXCbyPtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Scaling Non Categorical Data**"
      ],
      "metadata": {
        "id": "SUlw2x1dxfmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling categorical data\n",
        "sc = MinMaxScaler()\n",
        "a = sc.fit_transform(X[['Tenure Months']])\n",
        "b = sc.fit_transform(X[['Monthly Charges']])"
      ],
      "metadata": {
        "id": "DKKht2tpyeEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X['Tenure Months'] = a\n",
        "X['Monthly Charges'] = b"
      ],
      "metadata": {
        "id": "2GAKLjmd1Tu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop(\"Total Charges\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ahyd7hZz1lco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Upsampling the Data to Create a Balance between the two output Classs**"
      ],
      "metadata": {
        "id": "aQjcrL5Xx9C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting variable y to the churn value\n",
        "y = data[\"Churn Value\"]\n",
        "y\n",
        "\n",
        "#including churn prediction back so we can upsample it to balance out\n",
        "X[\"Churn Value\"] = y"
      ],
      "metadata": {
        "id": "uf4Z6-6P4JST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Splitting the Dataset into yes or no**"
      ],
      "metadata": {
        "id": "yXQKKirryMxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the dataset into yes or no\n",
        "x_no = X[X[\"Churn Value\"] == 0]\n",
        "x_yes = X[X[\"Churn Value\"] == 1]"
      ],
      "metadata": {
        "id": "nhVSn37z6W4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_no))\n",
        "print(len(x_yes))"
      ],
      "metadata": {
        "id": "bIm-HAO97qZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Upsampling the yes dataset to be the same size as the no dataset**"
      ],
      "metadata": {
        "id": "-Y8ZiQAwycx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upsampling the yes dataset to be the samw size as the no dataset\n",
        "x_yes_upsampled = x_yes.sample(n=len(x_no), replace=True, random_state=42)"
      ],
      "metadata": {
        "id": "hwvL8zGM5DGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Joining the Dataset**"
      ],
      "metadata": {
        "id": "8vEjsmmAypcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining both dataset\n",
        "x_upsampled = x_no.append(x_yes_upsampled).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jKVaz-cS8as-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x_upsampled[\"Churn Value\"]\n",
        "x_upsampled.drop(\"Churn Value\", axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "eTXGONxy9Wnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_upsampled"
      ],
      "metadata": {
        "id": "EMqT2KGr-Z1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "F4M6SM_P3zhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Converting y label to Categorical**"
      ],
      "metadata": {
        "id": "AQPi6bUVzWSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to convert he y label to categorical\n",
        "y = tf.keras.utils.to_categorical(y)"
      ],
      "metadata": {
        "id": "WVPlhsX6xuRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "z8JBJneqyM7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Splitting the dataset into training and testing**"
      ],
      "metadata": {
        "id": "Xu8uDayMzrhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the dataset into training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_upsampled, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "TxfuGzx72eRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Highlight of what to do at the next stage**\n",
        "\n",
        "* Use Logistic regression and plot the result\n",
        "* knearest neighbour and plot the accuracy, precision and recall\n",
        "* Use support vector machine (svm) and plot the result\n",
        "* Use Deep learning and plot the result\n"
      ],
      "metadata": {
        "id": "MvELo2T3qhGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Building The training Model**"
      ],
      "metadata": {
        "id": "2plGmr80_IgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**We have put all the Data preprocessing in one file, let's import it**"
      ],
      "metadata": {
        "id": "bQUhMccg56Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the python file that contain the data preprocessing function\n",
        "from drive.MyDrive import customer_churnfuction"
      ],
      "metadata": {
        "id": "YAsWokJi6DHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the function\n",
        "x_train, x_test, y_train, y_test = customer_churnfuction.perform_data_preprocessing(\"drive/MyDrive/Colab Notebooks/Telco_customer_churn.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd1KHMY4yw3t",
        "outputId": "1d35813d-36f5-438d-ef74-56afa62d1fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/customer_churnfuction.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  x_upsampled = x_no.append(x_yes_upsampled).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Logistic Regression**\n",
        "\n",
        "Logistic regression is a statistical algorithm used for binary classification tasks. It predicts the probability of an instance belonging to a certain class based on input features. Despite its name, it is primarily used for classification, not regression.\n",
        "\n",
        "In logistic regression, we start with a linear model that combines the input features linearly. The linear model can be represented as:\n",
        "\n",
        "z = b₀ + b₁x₁ + b₂x₂ + ... + bₚxₚ\n",
        "\n",
        "Here, z is the linear combination of the input features (x₁, x₂, ..., xₚ), b₀ is the bias term, and b₁, b₂, ..., bₚ are the coefficients (weights) associated with each feature.\n",
        "\n",
        "To map the linear output (z) to a probability value between 0 and 1, logistic regression uses the sigmoid function (also known as the logistic function). The sigmoid function is defined as:\n",
        "\n",
        "σ(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "In the equation, e represents the base of the natural logarithm, and -z is the input to the exponential function. The sigmoid function takes the linear output (z) and squashes it into a value between 0 and 1, representing the estimated probability of the positive class.\n",
        "\n",
        "During the training process, logistic regression aims to find the best set of coefficients (b₀, b₁, ..., bₚ) that maximizes the likelihood of the observed data. This is done by minimizing a cost function called log loss (also known as cross-entropy loss). The log loss penalizes large differences between the predicted probabilities and the true labels.\n",
        "\n",
        "Optimization algorithms, such as gradient descent or its variants, are used to minimize the cost function and find the optimal coefficients. These algorithms iteratively update the coefficients based on the gradient (derivative) of the cost function with respect to the coefficients. The process continues until convergence is reached, indicating that the coefficients have reached their optimal values.\n"
      ],
      "metadata": {
        "id": "R91RaJEamcZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Function to Train, Evaluate and Plot Result of Logistic Regression**"
      ],
      "metadata": {
        "id": "oxN4Mgo3-RSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_logistic_regression(x_train, y_train, x_test, y_test):\n",
        "    # Create an instance of the logistic regression model\n",
        "    logistic_regression = LogisticRegression()\n",
        "\n",
        "    # Fit the model\n",
        "    logistic_regression.fit(x_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = logistic_regression.predict(x_test)\n",
        "    y = tf.keras.utils.to_categorical(y_pred)\n",
        "\n",
        "    # Calculate accuracy, precision, and recall\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Print classification report\n",
        "    print(f\"The accuracy is {accuracy}\")\n",
        "    print(f\"The precision is {precision}\")\n",
        "    print(f\"The recall is {recall}\")\n",
        "\n",
        "    # Create a dictionary for the graph\n",
        "    datapoint = {\"accuracy\" : accuracy, \"precision\" : precision, \"recall\" : recall}\n",
        "\n",
        "    # Create a figure with three subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    # Plot the accuracy, precision, and recall\n",
        "    axs[0].bar(list(datapoint.keys()), list(datapoint.values()))\n",
        "    axs[0].set_title(\"Accuracy, Precision and Recall score for Logistic Regression\")\n",
        "    axs[0].bar[0].set_color('green')\n",
        "    axs[0].bar[1].set_color('black')\n",
        "    axs[0].bar[2].set_color('red')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, title='Confusion Matrix for Customer Churn prediction Logistic Regression optimized', ax=axs[1])\n",
        "\n",
        "    # Plot the precision-recall curve\n",
        "    skplt.metrics.plot_precision_recall(y_test, y, title='PR Curve for customer churn prediction Logistic Regression optimized', ax=axs[2])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "evaluate_logistic_regression(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "8xRfGAlv9KwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Optimizing Logistic Regression and Plotting the Result**"
      ],
      "metadata": {
        "id": "MERi-jw6_tei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_and_evaluate_logistic_regression(x_train, y_train, x_test, y_test):\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2', \"none\"]}\n",
        "\n",
        "    # Create an instance of LogisticRegression\n",
        "    logistic_regression = LogisticRegression()\n",
        "\n",
        "    # Create an instance of GridSearchCV\n",
        "    grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Print the best parameters found by GridSearchCV\n",
        "    print(\"Best Parameters: \", grid_search.best_params_)\n",
        "\n",
        "    # Predict on the test set using the best model\n",
        "    y_pred = grid_search.predict(x_test)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y = tf.keras.utils.to_categorical(y_pred)\n",
        "\n",
        "    # Calculate accuracy, precision, and recall\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"The accuracy is {accuracy}\")\n",
        "    print(f\"The precision is {precision}\")\n",
        "    print(f\"The recall is {recall}\")\n",
        "\n",
        "    # Create a dictionary for the graph\n",
        "    datapoint = {\"accuracy\" : accuracy, \"precision\" : precision, \"recall\" : recall}\n",
        "\n",
        "    # Create a figure with three subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    # Plot the accuracy, precision, and recall\n",
        "    axs[0].bar(list(datapoint.keys()), list(datapoint.values()))\n",
        "    axs[0].set_title(\"Accuracy, Precision and Recall score for Logistic Regression\")\n",
        "    axs[0].bar[0].set_color('green')\n",
        "    axs[0].bar[1].set_color('black')\n",
        "    axs[0].bar[2].set_color('red')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, title='Confusion Matrix for Customer Churn prediction Logistic Regression optimized', ax=axs[1])\n",
        "\n",
        "    # Plot the precision-recall curve\n",
        "    skplt.metrics.plot_precision_recall(y_test, y, title='PR Curve for customer churn prediction Logistic Regression optimized', ax=axs[2])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "tune_and_evaluate_logistic_regression(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "rhXRP3FV_nM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training Testing and Plotting Support Vector Machine**"
      ],
      "metadata": {
        "id": "xSjBRO21Am7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_svm(x_train, y_train, x_test, y_test):\n",
        "    # Create an SVM classifier\n",
        "    clf = svm.SVC(kernel='linear')\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "    y = tf.keras.utils.to_categorical(y_pred)\n",
        "\n",
        "    # Calculate accuracy, precision, and recall\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"The accuracy is {accuracy}\")\n",
        "    print(f\"The precision is {precision}\")\n",
        "    print(f\"The recall is {recall}\")\n",
        "\n",
        "    # Create a dictionary for the graph\n",
        "    datapoint = {\"accuracy\" : accuracy, \"precision\" : precision, \"recall\" : recall}\n",
        "\n",
        "    # Create a figure with three subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    # Plot the accuracy, precision, and recall\n",
        "    axs[0].bar(list(datapoint.keys()), list(datapoint.values()))\n",
        "    axs[0].set_title(\"Accuracy, Precision and Recall score for SVM\")\n",
        "    axs[0].bar[0].set_color('green')\n",
        "    axs[0].bar[1].set_color('black')\n",
        "    axs[0].bar[2].set_color('red')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, title='Confusion Matrix for Customer Churn prediction SVM', ax=axs[1])\n",
        "\n",
        "    # Plot the precision-recall curve\n",
        "    skplt.metrics.plot_precision_recall(y_test, y, title='PR Curve for customer churn prediction SVM', ax=axs[2])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "train_and_evaluate_svm(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "8QGFXWORAk3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}